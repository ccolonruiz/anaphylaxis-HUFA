{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk.data\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.stem import SnowballStemmer\n",
    "from scipy.sparse import find\n",
    "import scipy.sparse as sparse\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 What is the goal of the BagOfCentroids script?\n",
    "This script focuses on collecting data from the Extract_data script, as well as loading or training the W2V and Kmeans models. The objective is to generate a bag of centroids, from which we can create instances of the same length regardless of the size of the documents contained in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning the stop words of the text\n",
    "This time, to generate our training set, we will eliminate the stopwords in order to suppress the noise in the algorithm that will later be used to classify the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(raw_review, stemmer=False):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove non-letters\n",
    "    #letters_only = re.sub(\"[^A-Za-z0-9]\", \" \", review_text) \n",
    "    letters_only = re.sub(\"[^\\w\\d]\", \" \", raw_review) \n",
    "    #\n",
    "    # 2. Split into individual words\n",
    "    #### Para este modelo W2V no modificamos las mayúsculas\n",
    "    words = letters_only.split()\n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"spanish\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words and apply or not stemming\n",
    "    if stemmer:\n",
    "        meaningful_words = [stemmer.stem(w) for w in words if not w in stops]\n",
    "    else:\n",
    "        # \"re.sub(\"^\\d+$\", \"DIGITO\", w) Change all numbers with the token “DIGITO”\n",
    "        meaningful_words = [re.sub(\"^\\d+$\", \"DIGITO\", w) for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    #return( \" \".join( meaningful_words ))\n",
    "    return meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loading the W2V model\n",
    "This function is intended to load a W2V model. This allows us to obtain the word vectors that will be used to calculate the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_W2V_model(path):\n",
    "    model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    print(\"Loaded W2V model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training Kmeans model\n",
    "Once we have the words vectors, we are able to train a kmeans model. The objective is to make clusters in an unsupervised way and allow us to generate the data set we need for the classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_kmeans(model, path, wpc):\n",
    "    start = time.time() # Start time\n",
    "\n",
    "    # Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "    # vhufa average of 15 words per cluster\n",
    "    # wiki average of 280 words per cluster\n",
    "    word_vectors = model.vectors.astype(np.float64)\n",
    "    num_clusters = int(word_vectors.shape[0] / wpc)\n",
    "\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    # kmeans_clustering = KMeans( n_clusters = num_clusters ) # high memory consumption\n",
    "    kmeans_clustering = MiniBatchKMeans(n_clusters=num_clusters, batch_size=num_clusters)\n",
    "    kmeans_clustering.fit( word_vectors )\n",
    "\n",
    "    # save the model kmeans_clustering\n",
    "    with open(path, 'wb') as fid:\n",
    "        cPickle.dump(kmeans_clustering, fid)\n",
    "\n",
    "    idx = kmeans_clustering.predict( word_vectors )\n",
    "\n",
    "    # Get the end time and print how long the process took\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")\n",
    "\n",
    "    # Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "    # a cluster number                                                                                            \n",
    "    #word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "    return dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Loading Kmeans model\n",
    "Once we have the words vectors, we are able to train a kmeans model. With this function we can load a previously trained kmeans model. The objective is to make clusters in an unsupervised way and allow us to generate the data set we need for the classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_kmeans(model, path):\n",
    "    word_vectors = model.vectors.astype(np.float64)\n",
    "    with open(path, 'rb') as fid:\n",
    "        km_model = cPickle.load(fid)\n",
    "    print(\"Loaded Kmeans model\")\n",
    "    idx = km_model.predict( word_vectors )\n",
    "    return dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Creating bag of centroids\n",
    "So far, we focused on getting the different clusters, but in this part we will create the instances of each document. For this, we obtain to which cluster belongs each word from the text and represent all the attributes as the number of times the words appear in each one of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=np.uint16 )\n",
    "    #    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W2V model\n",
      "Loaded Kmeans model\n",
      "Loaded dataset\n"
     ]
    }
   ],
   "source": [
    "wpc = 10\n",
    "path_W2V = \"../W2V/sbw_vectors.bin\"\n",
    "path_Kmeans = \"../KMeans/sbw_vectors.pkl\"\n",
    "path_data = \"../hufa_test_wiki_w2v/test\"\n",
    "path_centroids = \"../hufa_test_wiki_w2v/minitest_centroids_test.npz\"\n",
    "#stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "word_centroid_map = load_kmeans(load_W2V_model(path_W2V), path_Kmeans)\n",
    "#word_centroid_map = train_kmeans(load_W2V_model(path_W2V), path_Kmeans, wpc)\n",
    "df = pd.read_csv(path_data, sep='\\t', index_col=0)\n",
    "print (\"Loaded dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "num_clusters = int(len(word_centroid_map)/wpc)\n",
    "clean_train_reviews = [review_to_wordlist( text, stemmer ) for text in df[\"text\"][ids_df]]\n",
    "\n",
    "train_centroids = None\n",
    "    \n",
    "for review in clean_train_reviews:\n",
    "    train_centroids = sparse.vstack([train_centroids, sparse.csr_matrix(create_bag_of_centroids( review, word_centroid_map ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new instances...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving new instances...\")\n",
    "#np.save(path_centroids, np.concatenate((train_centroids, np.array([df.label]).T), axis=1))\n",
    "# train_centroids = sparse.load_npz(\"../hufa_train_wiki_w2v/scentroids_train.npz\")\n",
    "sparse.save_npz(path_centroids, train_centroids)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hstack Xtest, Ytest or Xtrain, Ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_centroids = sparse.load_npz(\"../hufa_train_wiki_w2v/centroids_train.npz\")\n",
    "test_centroids = sparse.load_npz(\"../hufa_test_wiki_w2v/centroids_test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset\n"
     ]
    }
   ],
   "source": [
    "path_data_test = \"../hufa_test_wiki_w2v/test\"\n",
    "path_data_train = \"../hufa_train_wiki_w2v/train\"\n",
    "df_test = pd.read_csv(path_data_test, sep='\\t', index_col=0)\n",
    "df_train = pd.read_csv(path_data_train, sep='\\t', index_col=0)\n",
    "print (\"Loaded dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytest = sparse.csr_matrix(np.array(df_test[\"label\"]).reshape(df_test[\"label\"].shape[0],1))\n",
    "Xtest = sparse.csr_matrix(test_centroids)\n",
    "test_centroids = sparse.hstack([Xtest, Ytest]).tocsr()\n",
    "\n",
    "Ytrain = sparse.csr_matrix(np.array(df_train[\"label\"]).reshape(df_train[\"label\"].shape[0],1))\n",
    "Xtrain = sparse.csr_matrix(train_centroids)\n",
    "train_centroids = sparse.hstack([Xtrain, Ytrain]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse.save_npz(\"../hufa_train_wiki_w2v/full_centroids_train.npz\", train_centroids)\n",
    "sparse.save_npz(\"../hufa_test_wiki_w2v/full_centroids_test.npz\", test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_p_row_index = test_centroids[:,-1].nonzero()[0]\n",
    "# test_n_row_index = np.delete(np.arange(0, test_centroids.shape[0]), test_p_row_index)\n",
    "# test_positivos = test_centroids[test_p_row_index,:]\n",
    "# test_negativos = test_centroids[test_n_row_index,:]\n",
    "\n",
    "# train_p_row_index = train_centroids[:,-1].nonzero()[0]\n",
    "# train_n_row_index = np.delete(np.arange(0, train_centroids.shape[0]), train_p_row_index)\n",
    "# train_positivos = train_centroids[train_p_row_index,:]\n",
    "# train_negativos = train_centroids[train_n_row_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_df = [28673, 54473, 54478, 54482, 54501, 54508, 54509, 54532, 54539,\n",
    "       54558, 54562, 54580, 54585, 54617, 54632, 54637, 54639, 54641,\n",
    "       54643, 54657, 54696, 54698, 54715, 54730, 54746, 54767, 54769,\n",
    "       54770, 54774, 54791, 54814, 54827, 54830, 54838, 54841, 54844,\n",
    "       54848, 54852, 54855, 54876, 54917, 54926, 54928, 54932]\n",
    "len(df[\"text\"][ids_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44, 3827), (44, 1), (44, 3828))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_centroids.shape, Ytest.shape, test_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytest = sparse.csr_matrix(np.array(df[\"label\"][ids_df]).reshape(df[\"label\"][ids_df].shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest = sparse.csr_matrix(train_centroids)\n",
    "test_centroids = sparse.hstack([Xtest, Ytest]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.iloc[ids_df].to_csv(\"../hufa_test_stem_skip/fn_samples\", sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
