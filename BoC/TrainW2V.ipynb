{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unicodedata\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leyendo datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path_data = \"../Fulldata/clear_labeled_data\"\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "#df = pd.read_csv(path_data, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_old_data = \"../notas/Uurgencias\"\n",
    "path_new_data = \"../notas/Ualergias\"\n",
    "data = pd.concat([pd.read_csv(path_old_data, sep='\\t', index_col=0), pd.read_csv(path_new_data, sep='\\t', index_col=0)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función para obtencion de sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, stemmer=False):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    #sentences = []\n",
    "    #for raw_sentence in raw_sentences:\n",
    "    #    # If a sentence is empty, skip it\n",
    "    #    if len(raw_sentence) > 0:\n",
    "    #        # Otherwise, call review_to_wordlist to get a list of words\n",
    "    #        sentences.append(review_to_wordlist(raw_sentence))\n",
    "    #\n",
    "    if stemmer:\n",
    "        sentences = [review_to_wordlist(raw_sentence, stemmer) for raw_sentence in raw_sentences if len(raw_sentence) > 0]\n",
    "    else:\n",
    "        sentences = [review_to_wordlist(raw_sentence) for raw_sentence in raw_sentences if len(raw_sentence) > 0]\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función para el procesamiento de las sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(raw_review, stemmer=False):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove accent marks\n",
    "    review_text = ''.join((c for c in unicodedata.normalize('NFD',str(raw_review)) if unicodedata.category(c) != 'Mn'))\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    #letters_only = re.sub(\"[^A-Za-z0-9]\", \" \", review_text) \n",
    "    letters_only = re.sub(\"[^\\w\\d]\", \" \", review_text) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"spanish\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words and apply or not stemming\n",
    "    if stemmer:\n",
    "        meaningful_words = [stemmer.stem(w) for w in words if not w in stops]\n",
    "    else:\n",
    "        meaningful_words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    #return( \" \".join( meaningful_words ))\n",
    "    return meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiando todo el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished, Time taken:  2803.5532009601593 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # Start time\n",
    "sentences = []\n",
    "stem_sentences = []\n",
    "for review in data.text:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    stem_sentences += review_to_sentences(review, tokenizer, stemmer)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Finished, Time taken: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4855308, 4855308)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(stem_sentences) #(4776079, 4776079) (4855308, 4855308)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generando el Modelo de W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Finished, Time taken:  789.9282767772675 seconds.\n"
     ]
    }
   ],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print (\"Training model...\")\n",
    "start = time.time() # Start time\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg=1)\n",
    "stem_model = word2vec.Word2Vec(stem_sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg=1)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "model.init_sims(replace=True)\n",
    "stem_model.init_sims(replace=True)\n",
    "print(\"Finished, Time taken: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52497, 38274)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index2word), len(stem_model.wv.index2word) # (52497, 38274)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_W2V_stem = \"../W2V/hufa_stem-300-5w-10n-skip.bin\"\n",
    "path_W2V_nost = \"../W2V/hufa_nost-300-5w-10n-skip.bin\"\n",
    "stem_model.wv.save_word2vec_format(path_W2V_stem, binary=True)\n",
    "model.wv.save_word2vec_format(path_W2V_nost, binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
