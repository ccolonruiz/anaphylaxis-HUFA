{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk.data\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.stem import SnowballStemmer\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#parallel\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "#classifiers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define \"split_into_tokens\", function to process the text giving as result a list of tokens where the following steps have been made:\n",
    "<li> Accents are removed\n",
    "<li> Non-alphanumeric characters are filtered\n",
    "<li> Shift to lower case and split text in tokens\n",
    "<li> Deleted stopwords and replacement of the remaining words by their root (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(raw_review, stemmer=False):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove non-letters\n",
    "    #letters_only = re.sub(\"[^A-Za-z0-9]\", \" \", review_text) \n",
    "    letters_only = re.sub(\"[^\\w\\d]\", \" \", raw_review) \n",
    "    #\n",
    "    # 2. Split into individual words\n",
    "    #### Para este modelo W2V no modificamos las mayúsculas\n",
    "    words = letters_only.split()\n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"spanish\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words and apply or not stemming\n",
    "    if stemmer:\n",
    "        meaningful_words = [stemmer.stem(w) for w in words if not w in stops]\n",
    "    else:\n",
    "        # \"re.sub(\"^\\d+$\", \"DIGITO\", w) Change all numbers with the token “DIGITO”\n",
    "        meaningful_words = [re.sub(\"^\\d+$\", \"DIGITO\", w) for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    #return( \" \".join( meaningful_words ))\n",
    "    return meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function in charge of loading into memory a W2V model of an indicated route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_W2V_model(path):\n",
    "    model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    print(\"Loaded W2V model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function in charge of cleaning the full text, returning a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeclearlist(text):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    print(\"Limpiando texto. shape:\", text.shape)\n",
    "    clean_text = [review_to_wordlist( review ) for review in text]\n",
    "    print(\"Texto limpio. shape:\", text.shape)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"trainModel\" function receives the following arguments: the name of the classification algorithm, the class, its parameters, and the data sets. This function trains the model and returns a tuple with the name of the algorithm and the model already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(name, clazz, params, Xtrain, Xtrain_sca, Ytrain):\n",
    "    print(\"training \", name)\n",
    "    model = clazz(**params)\n",
    "    start = time.time() # Start time\n",
    "    if name == \"MultinomialNB\":\n",
    "        model.fit(Xtrain_sca, Ytrain)\n",
    "    else:\n",
    "        model.fit(Xtrain, Ytrain)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"-> done \", name, \" - Time taken for training:\", elapsed, \"seconds\")\n",
    "    return (name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"makeFeatureVec\" and \"getAvgFeatureVec\" functions are in charge of transforming each of the reviews into a vector of length \"num_features\".<br>.\n",
    "The resulting vector contains the average of all the vectors that represent each word contained in the vocabulary of the W2V model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords != 0:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    print (\"sub_clean_reviews len:\", len(reviews))\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%20000 == 0:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_voting(lista):\n",
    "    return np.argmax(np.bincount(lista))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into memory\n",
    "In the following section, we load the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets were loaded 164926 54976\n",
      "Loaded W2V model\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train', sep='\\t', index_col=0)\n",
    "df_test = pd.read_csv('./data/test', sep='\\t', index_col=0)\n",
    "\n",
    "Xtrain=df_train['text']\n",
    "Ytrain=df_train['label']\n",
    "\n",
    "Xtest=df_test['text']\n",
    "Ytest=df_test['label']\n",
    "print('datasets were loaded', len(df_train), len(df_test))\n",
    "\n",
    "num_features = 300\n",
    "path_W2V = \"W2V/sbw_vectors.bin\"\n",
    "model_W2V = load_W2V_model(path_W2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Classical\" algorithms\n",
    "In this section, we create a dictionary that contains all the necessary data of the classification algorithms that are going to be used, as well as the parameters of each one of them. (In case the parameters are not indicated, those configured by default in scikit-learn are used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = {\"KNeighbors\": (KNeighborsClassifier, {}),\n",
    "              \"MultinomialNB\" : (MultinomialNB, {}),\n",
    "              \"RandomForest\" : (RandomForestClassifier, {\"n_estimators\":100}),\n",
    "              \"LogisticRegression\" : (LogisticRegression, {}),\n",
    "              \"MLP\" : (MLPClassifier, {\"hidden_layer_sizes\":100}),\n",
    "              \"SVM\" : (SVC, {\"cache_size\":1000}),\n",
    "              \"LinearSVC\" : (LinearSVC, {})\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data set formed by the mean of the vectors\n",
    "<li> Clean the original text, both the train and the test\n",
    "<li> Convert the text, once cleaned, to the final sets of test and train composed by the average of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpiando texto. shape: (164926,)\n",
      "Limpiando texto. shape: (54976,)\n",
      "Texto limpio. shape: (54976,)\n",
      "Texto limpio. shape: (164926,)\n"
     ]
    }
   ],
   "source": [
    "clean_reviews = Parallel(n_jobs=2)(delayed(makeclearlist)(text) for text in [Xtrain, Xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_clean_reviews len: 164926\n",
      "Review 0 of 164926\n",
      "sub_clean_reviews len: 54976\n",
      "Review 0 of 54976\n",
      "Review 20000 of 164926\n",
      "Review 20000 of 54976\n",
      "Review 40000 of 164926\n",
      "Review 40000 of 54976\n",
      "Review 60000 of 164926\n",
      "Review 80000 of 164926\n",
      "Review 100000 of 164926\n",
      "Review 120000 of 164926\n",
      "Review 140000 of 164926\n",
      "Review 160000 of 164926\n"
     ]
    }
   ],
   "source": [
    "DataVecs = Parallel(n_jobs=2)(delayed(getAvgFeatureVecs)(sub_clean_reviews, model_W2V, num_features) for sub_clean_reviews in clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if DataVecs[0].shape[0] > DataVecs[1].shape[0]:\n",
    "    Xtrain_Avg = DataVecs[0]\n",
    "    Xtest_Avg = DataVecs[1]\n",
    "else:\n",
    "    Xtrain_Avg = DataVecs[1]\n",
    "    Xtest_Avg = DataVecs[0]\n",
    "Xtrain_Avg.shape, Xtest_Avg.shape\n",
    "\n",
    "# MultinomialNB: Input must be non-negative\n",
    "# To handle this problem, one of the possible solutions is to normalize all the data first\n",
    "Scaler = MinMaxScaler().fit(np.concatenate((Xtrain_Avg, Xtest_Avg), axis=0))\n",
    "Xtrain_Avg_sca = Scaler.transform(Xtrain_Avg)\n",
    "Xtest_Avg_sca = Scaler.transform(Xtest_Avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of classification models\n",
    "Next, we create as many processes as we have estimators. These processes will be in charge of training the different models in parallel.<br> \n",
    "As a result, we get a list of tuples formed by: (the name of the algorithm, the model already trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  MLP\n",
      "training  MultinomialNB\n",
      "-> done  MultinomialNB  - Time taken for training: 0.3875546455383301 seconds\n",
      "training  KNeighbors\n",
      "training  SVM\n",
      "training  LogisticRegression\n",
      "training  RandomForest\n",
      "training  LinearSVC\n",
      "-> done  KNeighbors  - Time taken for training: 10.216129779815674 seconds\n",
      "-> done  LinearSVC  - Time taken for training: 8.482576131820679 seconds\n",
      "-> done  LogisticRegression  - Time taken for training: 13.969268798828125 seconds\n",
      "-> done  MLP  - Time taken for training: 143.64605951309204 seconds\n",
      "-> done  SVM  - Time taken for training: 288.40018463134766 seconds\n",
      "-> done  RandomForest  - Time taken for training: 328.87237668037415 seconds\n"
     ]
    }
   ],
   "source": [
    "models = Parallel(n_jobs=len(estimators))(delayed(trainModel)(name, clazz, params, Xtrain_Avg, Xtrain_Avg_sca, Ytrain) for (name, (clazz, params)) in estimators.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models are trained, we obtain the labels from the test set and evaluate the results of each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo resultados:\n",
      "---------- Modelo:  MLP  ---------- Time taken for prediction: 2.2293083667755127 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9992    0.9994    0.9993     54473\n",
      "       True     0.9366    0.9105    0.9234       503\n",
      "\n",
      "avg / total     0.9986    0.9986    0.9986     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MultinomialNB  ---------- Time taken for prediction: 0.07517886161804199 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9909    1.0000    0.9954     54473\n",
      "       True     0.0000    0.0000    0.0000       503\n",
      "\n",
      "avg / total     0.9818    0.9909    0.9863     54976\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Modelo:  KNeighbors  ---------- Time taken for prediction: 3050.907576084137 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9991    0.9997    0.9994     54473\n",
      "       True     0.9598    0.9026    0.9303       503\n",
      "\n",
      "avg / total     0.9987    0.9988    0.9987     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  SVM  ---------- Time taken for prediction: 57.52249622344971 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9909    1.0000    0.9954     54473\n",
      "       True     0.0000    0.0000    0.0000       503\n",
      "\n",
      "avg / total     0.9818    0.9909    0.9863     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LogisticRegression  ---------- Time taken for prediction: 0.052895545959472656 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9979    0.9998    0.9989     54473\n",
      "       True     0.9678    0.7773    0.8622       503\n",
      "\n",
      "avg / total     0.9977    0.9977    0.9976     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  RandomForest  ---------- Time taken for prediction: 0.624234676361084 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9987    1.0000    0.9993     54473\n",
      "       True     1.0000    0.8549    0.9218       503\n",
      "\n",
      "avg / total     0.9987    0.9987    0.9986     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LinearSVC  ---------- Time taken for prediction: 0.052408456802368164 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9991    0.9997    0.9994     54473\n",
      "       True     0.9681    0.9046    0.9353       503\n",
      "\n",
      "avg / total     0.9988    0.9989    0.9988     54976\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Obteniendo resultados:\")\n",
    "results = []\n",
    "for (name, model) in models:\n",
    "    start = time.time() # Start time\n",
    "    if name == \"KNeighbors\":\n",
    "        result = [y for x in [Xtest_Avg[i:i+5000,:] for i in range(0,Xtest_Avg.shape[0],5000)] for y in model.predict(x)]\n",
    "    elif name == \"MultinomialNB\":\n",
    "        result = model.predict(Xtest_Avg_sca)\n",
    "    else:\n",
    "        result = model.predict(Xtest_Avg)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    results.append(list(result))\n",
    "    print(\"---------- Modelo: \", name, \" ---------- Time taken for prediction:\", elapsed,\"seconds\\n\", classification_report(Ytest, result, digits=4), \"\\n\")\n",
    "result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Modelo: Voting ---------- Time taken for prediction: 0.2185978889465332 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9991    1.0000    0.9995     54473\n",
      "       True     0.9978    0.8986    0.9456       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9990     54976\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # Start time\n",
    "result_voting = [simple_voting([x[i] for x in results]) for i in range(0, len(results[0]))]\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"---------- Modelo: Voting ---------- Time taken for prediction:\", elapsed,\"seconds\\n\", classification_report(Ytest, result_voting, digits=4), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
