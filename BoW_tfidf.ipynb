{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos\n",
    "\n",
    "**¿Qué representacones deberíamos a comparar?**\n",
    "<ul>\n",
    "<li>Bag of words (sin balancear, con todos los algoritmos \"clásicos\")</li>\n",
    "<li>Tf-idf model (sin balancear, con todos los algoritmos \"clásicos\"</li>\n",
    "\n",
    "-----\n",
    "\n",
    "<li>Documento representado como Average of word embeddings usando modelo w2v entrenado sobre EMR (con todos los algoritmos clásicos y sin balancear)</li>\n",
    "\n",
    "<li>Documento representado como Average of word embeddings usando modelo w2v entrenado sobre Spanish wikipedia (http://crscardellino.me/SBWCE/, https://github.com/Kyubyong/wordvectors). Tenemos que mirar si esos modelos preentrenados hacen stemming o no. Seguramente no lo hagan, y por tanto, nosotros no vamos a hacer stemming (con todos los algoritmos clásicos y sin balancear)</li>\n",
    "\n",
    "-----\n",
    "\n",
    "<li>bag of centroids (sobre el modelo de EMRs)</li>\n",
    "<li>bag of centroids (sobre el modelo de wikipedia)</li>\n",
    "</ul>\n",
    "\n",
    "Mi idea es quitar ElasticSearch y dejar un bseline de bolsa de palabras \n",
    "\n",
    "**¿Qué métodos (clasificadores) deberíamos a comparar?**\n",
    "\n",
    "<ul>\n",
    "<li>Classifiers: **SVM, MLP, RandomForest, MultinomialNB**, KNeighborsClassifier, AdaBoostClassifier, GaussianNB, QuadraticDiscriminantAnalysis</li>\n",
    "\n",
    "<li>Deberíamos encontrar la mejor setting para cada uno de ellos? GridSearchCV </li>\n",
    "\n",
    "<li>Ensemble (Voting): SVM, MLP, RandomForest, MultinomialNB, KNeighborsClassifier, AdaBoostClassifier, GaussianNB, QuadraticDiscriminantAnalysis</li>\n",
    "<li>CNN: malos resultados</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import time\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "\n",
    "#classifiers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "#graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#parallel\n",
    "from sklearn.externals.joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos en memoria\n",
    "En la siguiente sección cargamos los datos de entrenamiento y de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets were loaded 164926 54976\n"
     ]
    }
   ],
   "source": [
    "df_train = pandas.read_csv('./data/train', sep='\\t', index_col=0)\n",
    "df_test = pandas.read_csv('./data/test', sep='\\t', index_col=0)\n",
    "\n",
    "Xtrain=df_train['text']\n",
    "Ytrain=df_train['label']\n",
    "\n",
    "Xtest=df_test['text']\n",
    "Ytest=df_test['label']\n",
    "print('datasets were loaded', len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos \"split_into_tokens\", función encargada de procesar el texto dando como resultado una lista de tokens donde se han relizado los siguientes pasos:\n",
    "<li> Los signos de acentuación son eliminados\n",
    "<li> Los caracteres no alfanuméricos son filtrados\n",
    "<li> Mayus a minus y split del texto en tokens\n",
    "<li> Eliminadas las stopwords y sustitución de las palabras restantes por su raiz (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_tokens(text):\n",
    "    #stemmer = SnowballStemmer('spanish')\n",
    "    stemmer = None\n",
    "    min_length = 3\n",
    "    # 1. Remove accent marks\n",
    "    review_text = ''.join((c for c in unicodedata.normalize('NFD',str(text)) if unicodedata.category(c) != 'Mn'))\n",
    "    #\n",
    "    # 2. Remove non-alphanumeric\n",
    "    #letters_only = re.sub(\"[^A-Za-z0-9]\", \" \", review_text) \n",
    "    letters_only = re.sub(\"[^\\w\\d]\", \" \", review_text) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"spanish\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words and apply or not stemming\n",
    "    if stemmer:\n",
    "        filtered_tokens = [stemmer.stem(w) for w in words if not w in stops and len(w)>=min_length]\n",
    "    else:\n",
    "        filtered_tokens = [w for w in words if not w in stops and len(w)>=min_length]\n",
    "    #\n",
    "    # 5. return the result\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disminucion', 'diuresis', 'claudicacion', 'mandibular', 'amaurosis']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba = \"Refiere intermitente sensacion de acorchamiento en zona frontal derecha y hemicara derecha\"\n",
    "prueba2 = \"No disminucion de la diuresis. No claudicacion mandibular. No amaurosis\"\n",
    "\n",
    "filtrado = split_into_tokens(prueba2)\n",
    "filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer=split_into_tokens)\n",
    "\n",
    "print(\"Creando matriz de bolsa de palabras...\")\n",
    "\n",
    "bow.fit(Xtrain, Ytrain)\n",
    "Xprueba_bow = bow.transform(Xtrain)\n",
    "\n",
    "Xprueba_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función \"trainModel\" recibe el nombre del algoritmo de clasificación, la clase, sus parametros y los conjuntos de datos. Dicha función se encarga de entrenar el modelo y devolver una tupla con el nombre del algoritmo y el modelo ya entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(name, clazz, params, Xtrain, Ytrain):\n",
    "    print(\"training \", name)\n",
    "    model = clazz(**params)\n",
    "    start = time.time() # Start time\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"-> done \", name, \" - Time taken for training:\", elapsed, \"seconds\")\n",
    "    return (name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos \"clásicos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección creamos un diccionario que contiene todos los datos necesarios de los algoritmos de clasificación que se van a emplear, así como los parametros de cada uno de ellos. (En caso de no estar indicados los parametros, se emplean aquellos configurados por defecto en scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = {\"KNeighbors\": (KNeighborsClassifier, {}),\n",
    "              \"MultinomialNB\" : (MultinomialNB, {}),\n",
    "              \"RandomForest\" : (RandomForestClassifier, {\"n_estimators\":100}),\n",
    "              \"LogisticRegression\" : (LogisticRegression, {}),\n",
    "              \"MLP\" : (MLPClassifier, {\"hidden_layer_sizes\":100}),\n",
    "              \"SVM\" : (SVC, {\"cache_size\":1000}),\n",
    "              \"LinearSVC\" : (LinearSVC, {})\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words (sin balancear, con todos los algoritmos \"clásicos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se transforman los datos de entrenamiento y de test en bolsa de palabras, pasando de un conjunto de tokens, a un conjunto de número de ocurrencias por cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de bolsa de palabras...\n",
      "CPU times: user 8min 42s, sys: 1.4 s, total: 8min 43s\n",
      "Wall time: 8min 43s\n",
      "CPU times: user 8min 43s, sys: 1.25 s, total: 8min 44s\n",
      "Wall time: 8min 44s\n",
      "CPU times: user 2min 53s, sys: 448 ms, total: 2min 53s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(analyzer=split_into_tokens)\n",
    "\n",
    "print(\"Creando matriz de bolsa de palabras...\")\n",
    "\n",
    "%time bow.fit(Xtrain, Ytrain)\n",
    "%time Xtrain_bow = bow.transform(Xtrain)\n",
    "%time Xtest_bow = bow.transform(Xtest)\n",
    "\n",
    "scipy.sparse.save_npz('data/Xtrain_bow.npz', Xtrain_bow)\n",
    "scipy.sparse.save_npz('data/Xtest_bow.npz', Xtest_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54976, 140425)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección carga en memoria las bolsas de palabras. Emplear solo en el caso de haber obtenido previamente las bolsas de palabras y no tenerlas cargadas en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xtest_bow = scipy.sparse.load_npz('data/Xtest_bow.npz').astype(np.int16, casting='same_kind')\n",
    "# Xtrain_bow = scipy.sparse.load_npz('data/Xtrain_bow.npz').astype(np.int16, casting='same_kind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, creamos tantos procesos como estimadores tengamos. Dichos procesos van a estar encargados de entrenar, cada uno y de forma paralela, un modelo de clasificación distinto.<br> \n",
    "Como resultado obtenemos una lista de tuplas formadas por: (el nombre del algoritmo, modelo ya entrenado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  KNeighbors\n",
      "-> done  KNeighbors  - Time taken for training: 0.05667233467102051 seconds\n",
      "training  MultinomialNB\n",
      "-> done  MultinomialNB  - Time taken for training: 0.1639249324798584 seconds\n",
      "training  MLP\n",
      "training  RandomForest\n",
      "training  LogisticRegression\n",
      "training  LinearSVC\n",
      "training  SVM\n",
      "-> done  LinearSVC  - Time taken for training: 2.9192557334899902 seconds\n",
      "-> done  LogisticRegression  - Time taken for training: 17.37110424041748 seconds\n",
      "-> done  RandomForest  - Time taken for training: 52.50565981864929 seconds\n",
      "-> done  SVM  - Time taken for training: 562.716991186142 seconds\n",
      "-> done  MLP  - Time taken for training: 2464.838773727417 seconds\n"
     ]
    }
   ],
   "source": [
    "models = Parallel(n_jobs=len(estimators))(delayed(trainModel)(name, clazz, params, Xtrain_bow, Ytrain) for (name, (clazz, params)) in estimators.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenados los modelos, obtenemos la clase de los individuos del conjunto de test y evaluamos los resultados de cada uno de ellos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo resultados:\n",
      "---------- Modelo:  KNeighbors  ---------- Time taken for prediction: 661.5408456325531 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9987    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8628    0.9264       503\n",
      "\n",
      "avg / total     0.9987    0.9987    0.9987     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MultinomialNB  ---------- Time taken for prediction: 0.11231493949890137 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9992    0.9976    0.9984     54473\n",
      "       True     0.7761    0.9165    0.8405       503\n",
      "\n",
      "avg / total     0.9972    0.9968    0.9969     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MLP  ---------- Time taken for prediction: 0.4970865249633789 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9996    0.9995     54473\n",
      "       True     0.9551    0.9304    0.9426       503\n",
      "\n",
      "avg / total     0.9990    0.9990    0.9990     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  RandomForest  ---------- Time taken for prediction: 2.705735921859741 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9989    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8807    0.9366       503\n",
      "\n",
      "avg / total     0.9989    0.9989    0.9989     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LogisticRegression  ---------- Time taken for prediction: 0.0226442813873291 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9997    0.9995     54473\n",
      "       True     0.9670    0.9324    0.9494       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LinearSVC  ---------- Time taken for prediction: 0.022754669189453125 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9997    0.9995     54473\n",
      "       True     0.9669    0.9304    0.9483       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  SVM  ---------- Time taken for prediction: 121.96885681152344 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9937    1.0000    0.9969     54473\n",
      "       True     1.0000    0.3181    0.4827       503\n",
      "\n",
      "avg / total     0.9938    0.9938    0.9922     54976\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Obteniendo resultados:\")\n",
    "for (name, model) in models:\n",
    "    start = time.time() # Start time\n",
    "    if name == \"KNeighbors\":\n",
    "        result = [y for x in [Xtest_bow[i:i+5000,:] for i in range(0,Xtest_bow.shape[0],5000)] for y in model.predict(x)]\n",
    "    else:\n",
    "        result = model.predict(Xtest_bow)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"---------- Modelo: \", name, \" ---------- Time taken for prediction:\", elapsed,\"seconds\\n\", classification_report(Ytest, result, digits=4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf model (sin balancear, con todos los algoritmos \"clásicos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar con los resultados obtenidos, emplearemos la representación de los datos de entrenamiento y test en formato de Tf-idf. Para ello, la bolsa de palabras es transformada en la frecuencia de ocurrencia de los términos en la colección de documentos.<br>\n",
    "El proceso que se lleva a cabo, a partir de este punto, es el mismo que el empleado con la bolsa de palabras anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de tf-idf...\n",
      "CPU times: user 64 ms, sys: 24 ms, total: 88 ms\n",
      "Wall time: 85.9 ms\n",
      "CPU times: user 412 ms, sys: 140 ms, total: 552 ms\n",
      "Wall time: 549 ms\n",
      "CPU times: user 144 ms, sys: 32 ms, total: 176 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "print(\"Creando matriz de tf-idf...\")\n",
    "\n",
    "%time tfidf.fit(Xtrain_bow, Ytrain)\n",
    "%time Xtrain_tfidf = tfidf.transform(Xtrain_bow)\n",
    "%time Xtest_tfidf = tfidf.transform(Xtest_bow)\n",
    "\n",
    "scipy.sparse.save_npz('data/Xtrain_tfidf.npz', Xtrain_tfidf)\n",
    "scipy.sparse.save_npz('data/Xtest_tfidf.npz', Xtest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección carga en memoria la representación de los datos en tf-idf. Emplear solo en el caso de haber obtenido previamente dichos datos y no tenerlos cargadas en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xtest_tfidf = scipy.sparse.load_npz('data/Xtest_tfidf.npz').astype(np.float32)\n",
    "# Xtrain_tfidf = scipy.sparse.load_npz('data/Xtrain_tfidf.npz').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<164926x140425 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 18552095 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, creamos tantos procesos como estimadores tengamos. Dichos procesos van a estar encargados de entrenar, cada uno y de forma paralela, un modelo de clasificación distinto.<br> \n",
    "Como resultado obtenemos una lista de tuplas formadas por: (el nombre del algoritmo, modelo ya entrenado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  KNeighbors\n",
      "training  MultinomialNB\n",
      "-> done  KNeighbors  - Time taken for training: 0.29846620559692383 seconds\n",
      "-> done  MultinomialNB  - Time taken for training: 0.17311930656433105 seconds\n",
      "training  MLP\n",
      "training  RandomForest\n",
      "training  LogisticRegression\n",
      "training  LinearSVC\n",
      "training  SVM\n",
      "-> done  LinearSVC  - Time taken for training: 1.6419150829315186 seconds\n",
      "-> done  LogisticRegression  - Time taken for training: 5.75917911529541 seconds\n",
      "-> done  RandomForest  - Time taken for training: 58.53112745285034 seconds\n",
      "-> done  SVM  - Time taken for training: 429.11979126930237 seconds\n",
      "-> done  MLP  - Time taken for training: 4001.9736495018005 seconds\n"
     ]
    }
   ],
   "source": [
    "models_tfidf = Parallel(n_jobs=len(estimators))(delayed(trainModel)(name, clazz, params, Xtrain_tfidf, Ytrain) for (name, (clazz, params)) in estimators.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenados los modelos, obtenemos la clase de los individuos del conjunto de test y evaluamos los resultados de cada uno de ellos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo resultados:\n",
      "---------- Modelo:  KNeighbors  ---------- Time taken for prediction: 632.7096681594849 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9977    1.0000    0.9988     54473\n",
      "       True     1.0000    0.7475    0.8555       503\n",
      "\n",
      "avg / total     0.9977    0.9977    0.9975     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MultinomialNB  ---------- Time taken for prediction: 0.03978276252746582 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9922    1.0000    0.9961     54473\n",
      "       True     1.0000    0.1451    0.2535       503\n",
      "\n",
      "avg / total     0.9922    0.9922    0.9893     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MLP  ---------- Time taken for prediction: 0.5144734382629395 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9997    0.9995     54473\n",
      "       True     0.9670    0.9324    0.9494       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  RandomForest  ---------- Time taken for prediction: 2.7270984649658203 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9987    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8628    0.9264       503\n",
      "\n",
      "avg / total     0.9987    0.9987    0.9987     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LogisticRegression  ---------- Time taken for prediction: 0.026710987091064453 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9992    0.9999    0.9996     54473\n",
      "       True     0.9913    0.9105    0.9492       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LinearSVC  ---------- Time taken for prediction: 0.026583433151245117 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9993    0.9999    0.9996     54473\n",
      "       True     0.9893    0.9205    0.9537       503\n",
      "\n",
      "avg / total     0.9992    0.9992    0.9992     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  SVM  ---------- Time taken for prediction: 124.67631936073303 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9909    1.0000    0.9954     54473\n",
      "       True     0.0000    0.0000    0.0000       503\n",
      "\n",
      "avg / total     0.9818    0.9909    0.9863     54976\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Obteniendo resultados:\")\n",
    "for (name, model) in models_tfidf:\n",
    "    start = time.time() # Start time\n",
    "    if name == \"KNeighbors\":\n",
    "        result = [y for x in [Xtest_tfidf[i:i+5000,:] for i in range(0,Xtest_tfidf.shape[0],5000)] for y in model.predict(x)]\n",
    "    else:\n",
    "        result = model.predict(Xtest_tfidf)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"---------- Modelo: \", name, \" ---------- Time taken for prediction:\", elapsed,\"seconds\\n\", classification_report(Ytest, result, digits=4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for training: 2449.9408764839172 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble = VotingClassifier(models, n_jobs=-1)\n",
    "start = time.time() # Start time\n",
    "voting_model_bow=ensemble.fit(Xtrain_bow,Ytrain)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for training:\", elapsed, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for prediction: 800.65008020401 seconds\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9992    0.9999    0.9996     54473\n",
      "       True     0.9935    0.9085    0.9491       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "start = time.time() # Start time\n",
    "predictions1 = [y for x in [Xtest_bow[i:i+2000,:] for i in range(0,Xtest_bow.shape[0],2000)] for y in voting_model_bow.predict(x)]\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for prediction:\", elapsed, \"seconds\")\n",
    "cr1=classification_report(Ytest, predictions1, digits=4)\n",
    "print(cr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting (Tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for training: 3767.6675279140472 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble = VotingClassifier(models_tfidf, n_jobs=-1)\n",
    "start = time.time() # Start time\n",
    "voting_model_tfidf=ensemble.fit(Xtrain_tfidf,Ytrain)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for training:\", elapsed, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for prediction: 731.4793696403503 seconds\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9988    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8728    0.9321       503\n",
      "\n",
      "avg / total     0.9988    0.9988    0.9988     54976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "start = time.time() # Start time\n",
    "predictions2 = [y for x in [Xtest_tfidf[i:i+2000,:] for i in range(0,Xtest_tfidf.shape[0],2000)] for y in voting_model_tfidf.predict(x)]\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for prediction:\", elapsed, \"seconds\")\n",
    "cr2=classification_report(Ytest, predictions2, digits=4)\n",
    "print(cr2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
