{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import time\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "\n",
    "#classifiers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "#graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#parallel\n",
    "from sklearn.externals.joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into memory\n",
    "In the following section, we load the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets were loaded 164926 54976\n"
     ]
    }
   ],
   "source": [
    "df_train = pandas.read_csv('./data/train', sep='\\t', index_col=0)\n",
    "df_test = pandas.read_csv('./data/test', sep='\\t', index_col=0)\n",
    "\n",
    "Xtrain=df_train['text']\n",
    "Ytrain=df_train['label']\n",
    "\n",
    "Xtest=df_test['text']\n",
    "Ytest=df_test['label']\n",
    "print('datasets were loaded', len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define \"split_into_tokens\", function to process the text giving as result a list of tokens where the following steps have been made:\n",
    "<li> Accents are removed\n",
    "<li> Non-alphanumeric characters are filtered\n",
    "<li> Shift to lower case and split text in tokens\n",
    "<li> Deleted stopwords and replacement of the remaining words by their root (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_tokens(text):\n",
    "    #stemmer = SnowballStemmer('spanish')\n",
    "    stemmer = None\n",
    "    min_length = 3\n",
    "    # 1. Remove accent marks\n",
    "    review_text = ''.join((c for c in unicodedata.normalize('NFD',str(text)) if unicodedata.category(c) != 'Mn'))\n",
    "    #\n",
    "    # 2. Remove non-alphanumeric\n",
    "    #letters_only = re.sub(\"[^A-Za-z0-9]\", \" \", review_text) \n",
    "    letters_only = re.sub(\"[^\\w\\d]\", \" \", review_text) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"spanish\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words and apply or not stemming\n",
    "    if stemmer:\n",
    "        filtered_tokens = [stemmer.stem(w) for w in words if not w in stops and len(w)>=min_length]\n",
    "    else:\n",
    "        filtered_tokens = [w for w in words if not w in stops and len(w)>=min_length]\n",
    "    #\n",
    "    # 5. return the result\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"trainModel\" function receives the following arguments: the name of the classification algorithm, the class, its parameters, and the data sets. This function trains the model and returns a tuple with the name of the algorithm and the model already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(name, clazz, params, Xtrain, Ytrain):\n",
    "    print(\"training \", name)\n",
    "    model = clazz(**params)\n",
    "    start = time.time() # Start time\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"-> done \", name, \" - Time taken for training:\", elapsed, \"seconds\")\n",
    "    return (name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Classical\" algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create a dictionary that contains all the necessary data of the classification algorithms that are going to be used, as well as the parameters of each one of them. (In case the parameters are not indicated, those configured by default in scikit-learn are used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = {\"KNeighbors\": (KNeighborsClassifier, {}),\n",
    "              \"MultinomialNB\" : (MultinomialNB, {}),\n",
    "              \"RandomForest\" : (RandomForestClassifier, {\"n_estimators\":100}),\n",
    "              \"LogisticRegression\" : (LogisticRegression, {}),\n",
    "              \"MLP\" : (MLPClassifier, {\"hidden_layer_sizes\":100}),\n",
    "              \"SVM\" : (SVC, {\"cache_size\":1000}),\n",
    "              \"LinearSVC\" : (LinearSVC, {})\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words  (unbalanced, with all \"classic\" algorithms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, training and test data are transformed into a bag of words, going from a set of tokens to a set of occurrences per token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de bolsa de palabras...\n",
      "CPU times: user 8min 42s, sys: 1.4 s, total: 8min 43s\n",
      "Wall time: 8min 43s\n",
      "CPU times: user 8min 43s, sys: 1.25 s, total: 8min 44s\n",
      "Wall time: 8min 44s\n",
      "CPU times: user 2min 53s, sys: 448 ms, total: 2min 53s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(analyzer=split_into_tokens)\n",
    "\n",
    "print(\"Creando matriz de bolsa de palabras...\")\n",
    "\n",
    "%time bow.fit(Xtrain, Ytrain)\n",
    "%time Xtrain_bow = bow.transform(Xtrain)\n",
    "%time Xtest_bow = bow.transform(Xtest)\n",
    "\n",
    "scipy.sparse.save_npz('data/Xtrain_bow.npz', Xtrain_bow)\n",
    "scipy.sparse.save_npz('data/Xtest_bow.npz', Xtest_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section, loads in memory the bags of words. Use only if you have previously obtained the bags of words and do not have them loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xtest_bow = scipy.sparse.load_npz('data/Xtest_bow.npz').astype(np.int16, casting='same_kind')\n",
    "# Xtrain_bow = scipy.sparse.load_npz('data/Xtrain_bow.npz').astype(np.int16, casting='same_kind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create as many processes as we have estimators. These processes will be in charge of training the different models in parallel.<br> \n",
    "As a result, we get a list of tuples formed by: (the name of the algorithm, the model already trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  KNeighbors\n",
      "-> done  KNeighbors  - Time taken for training: 0.05667233467102051 seconds\n",
      "training  MultinomialNB\n",
      "-> done  MultinomialNB  - Time taken for training: 0.1639249324798584 seconds\n",
      "training  MLP\n",
      "training  RandomForest\n",
      "training  LogisticRegression\n",
      "training  LinearSVC\n",
      "training  SVM\n",
      "-> done  LinearSVC  - Time taken for training: 2.9192557334899902 seconds\n",
      "-> done  LogisticRegression  - Time taken for training: 17.37110424041748 seconds\n",
      "-> done  RandomForest  - Time taken for training: 52.50565981864929 seconds\n",
      "-> done  SVM  - Time taken for training: 562.716991186142 seconds\n",
      "-> done  MLP  - Time taken for training: 2464.838773727417 seconds\n"
     ]
    }
   ],
   "source": [
    "models = Parallel(n_jobs=len(estimators))(delayed(trainModel)(name, clazz, params, Xtrain_bow, Ytrain) for (name, (clazz, params)) in estimators.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models are trained, we obtain the labels from the test set and evaluate the results of each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo resultados:\n",
      "---------- Modelo:  KNeighbors  ---------- Time taken for prediction: 661.5408456325531 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9987    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8628    0.9264       503\n",
      "\n",
      "avg / total     0.9987    0.9987    0.9987     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MultinomialNB  ---------- Time taken for prediction: 0.11231493949890137 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9992    0.9976    0.9984     54473\n",
      "       True     0.7761    0.9165    0.8405       503\n",
      "\n",
      "avg / total     0.9972    0.9968    0.9969     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MLP  ---------- Time taken for prediction: 0.4970865249633789 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9996    0.9995     54473\n",
      "       True     0.9551    0.9304    0.9426       503\n",
      "\n",
      "avg / total     0.9990    0.9990    0.9990     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  RandomForest  ---------- Time taken for prediction: 2.705735921859741 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9989    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8807    0.9366       503\n",
      "\n",
      "avg / total     0.9989    0.9989    0.9989     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LogisticRegression  ---------- Time taken for prediction: 0.0226442813873291 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9997    0.9995     54473\n",
      "       True     0.9670    0.9324    0.9494       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LinearSVC  ---------- Time taken for prediction: 0.022754669189453125 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9997    0.9995     54473\n",
      "       True     0.9669    0.9304    0.9483       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  SVM  ---------- Time taken for prediction: 121.96885681152344 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9937    1.0000    0.9969     54473\n",
      "       True     1.0000    0.3181    0.4827       503\n",
      "\n",
      "avg / total     0.9938    0.9938    0.9922     54976\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Obteniendo resultados:\")\n",
    "for (name, model) in models:\n",
    "    start = time.time() # Start time\n",
    "    if name == \"KNeighbors\":\n",
    "        result = [y for x in [Xtest_bow[i:i+5000,:] for i in range(0,Xtest_bow.shape[0],5000)] for y in model.predict(x)]\n",
    "    else:\n",
    "        result = model.predict(Xtest_bow)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"---------- Modelo: \", name, \" ---------- Time taken for prediction:\", elapsed,\"seconds\\n\", classification_report(Ytest, result, digits=4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf model (unbalanced, with all \"classic\" algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare with the results obtained, we will use the representation of the training and test data in Tf-idf format. To achieve this, the bag of words is transformed into the frequency of occurrence of terms in the collection of documents.<br>\n",
    "The process that is carried out, from this point on, is the same as the employee with the bag of words previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de tf-idf...\n",
      "CPU times: user 64 ms, sys: 24 ms, total: 88 ms\n",
      "Wall time: 85.9 ms\n",
      "CPU times: user 412 ms, sys: 140 ms, total: 552 ms\n",
      "Wall time: 549 ms\n",
      "CPU times: user 144 ms, sys: 32 ms, total: 176 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "print(\"Creando matriz de tf-idf...\")\n",
    "\n",
    "%time tfidf.fit(Xtrain_bow, Ytrain)\n",
    "%time Xtrain_tfidf = tfidf.transform(Xtrain_bow)\n",
    "%time Xtest_tfidf = tfidf.transform(Xtest_bow)\n",
    "\n",
    "scipy.sparse.save_npz('data/Xtrain_tfidf.npz', Xtrain_tfidf)\n",
    "scipy.sparse.save_npz('data/Xtest_tfidf.npz', Xtest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section, loads in memory the bags of words. Use only if you have previously obtained the bags of words and do not have them loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xtest_tfidf = scipy.sparse.load_npz('data/Xtest_tfidf.npz').astype(np.float32)\n",
    "# Xtrain_tfidf = scipy.sparse.load_npz('data/Xtrain_tfidf.npz').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<164926x140425 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 18552095 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create as many processes as we have estimators. These processes will be in charge of training the different models in parallel.<br> \n",
    "As a result, we get a list of tuples formed by: (the name of the algorithm, the model already trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  KNeighbors\n",
      "training  MultinomialNB\n",
      "-> done  KNeighbors  - Time taken for training: 0.29846620559692383 seconds\n",
      "-> done  MultinomialNB  - Time taken for training: 0.17311930656433105 seconds\n",
      "training  MLP\n",
      "training  RandomForest\n",
      "training  LogisticRegression\n",
      "training  LinearSVC\n",
      "training  SVM\n",
      "-> done  LinearSVC  - Time taken for training: 1.6419150829315186 seconds\n",
      "-> done  LogisticRegression  - Time taken for training: 5.75917911529541 seconds\n",
      "-> done  RandomForest  - Time taken for training: 58.53112745285034 seconds\n",
      "-> done  SVM  - Time taken for training: 429.11979126930237 seconds\n",
      "-> done  MLP  - Time taken for training: 4001.9736495018005 seconds\n"
     ]
    }
   ],
   "source": [
    "models_tfidf = Parallel(n_jobs=len(estimators))(delayed(trainModel)(name, clazz, params, Xtrain_tfidf, Ytrain) for (name, (clazz, params)) in estimators.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models are trained, we obtain the labels from the test set and evaluate the results of each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo resultados:\n",
      "---------- Modelo:  KNeighbors  ---------- Time taken for prediction: 632.7096681594849 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9977    1.0000    0.9988     54473\n",
      "       True     1.0000    0.7475    0.8555       503\n",
      "\n",
      "avg / total     0.9977    0.9977    0.9975     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MultinomialNB  ---------- Time taken for prediction: 0.03978276252746582 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9922    1.0000    0.9961     54473\n",
      "       True     1.0000    0.1451    0.2535       503\n",
      "\n",
      "avg / total     0.9922    0.9922    0.9893     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  MLP  ---------- Time taken for prediction: 0.5144734382629395 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9994    0.9997    0.9995     54473\n",
      "       True     0.9670    0.9324    0.9494       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  RandomForest  ---------- Time taken for prediction: 2.7270984649658203 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9987    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8628    0.9264       503\n",
      "\n",
      "avg / total     0.9987    0.9987    0.9987     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LogisticRegression  ---------- Time taken for prediction: 0.026710987091064453 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9992    0.9999    0.9996     54473\n",
      "       True     0.9913    0.9105    0.9492       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  LinearSVC  ---------- Time taken for prediction: 0.026583433151245117 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9993    0.9999    0.9996     54473\n",
      "       True     0.9893    0.9205    0.9537       503\n",
      "\n",
      "avg / total     0.9992    0.9992    0.9992     54976\n",
      " \n",
      "\n",
      "---------- Modelo:  SVM  ---------- Time taken for prediction: 124.67631936073303 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9909    1.0000    0.9954     54473\n",
      "       True     0.0000    0.0000    0.0000       503\n",
      "\n",
      "avg / total     0.9818    0.9909    0.9863     54976\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Obteniendo resultados:\")\n",
    "for (name, model) in models_tfidf:\n",
    "    start = time.time() # Start time\n",
    "    if name == \"KNeighbors\":\n",
    "        result = [y for x in [Xtest_tfidf[i:i+5000,:] for i in range(0,Xtest_tfidf.shape[0],5000)] for y in model.predict(x)]\n",
    "    else:\n",
    "        result = model.predict(Xtest_tfidf)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"---------- Modelo: \", name, \" ---------- Time taken for prediction:\", elapsed,\"seconds\\n\", classification_report(Ytest, result, digits=4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for training: 2449.9408764839172 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble = VotingClassifier(models, n_jobs=-1)\n",
    "start = time.time() # Start time\n",
    "voting_model_bow=ensemble.fit(Xtrain_bow,Ytrain)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for training:\", elapsed, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for prediction: 800.65008020401 seconds\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9992    0.9999    0.9996     54473\n",
      "       True     0.9935    0.9085    0.9491       503\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     54976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "start = time.time() # Start time\n",
    "predictions1 = [y for x in [Xtest_bow[i:i+2000,:] for i in range(0,Xtest_bow.shape[0],2000)] for y in voting_model_bow.predict(x)]\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for prediction:\", elapsed, \"seconds\")\n",
    "cr1=classification_report(Ytest, predictions1, digits=4)\n",
    "print(cr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting (Tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for training: 3767.6675279140472 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble = VotingClassifier(models_tfidf, n_jobs=-1)\n",
    "start = time.time() # Start time\n",
    "voting_model_tfidf=ensemble.fit(Xtrain_tfidf,Ytrain)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for training:\", elapsed, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier - Time taken for prediction: 731.4793696403503 seconds\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False     0.9988    1.0000    0.9994     54473\n",
      "       True     1.0000    0.8728    0.9321       503\n",
      "\n",
      "avg / total     0.9988    0.9988    0.9988     54976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "start = time.time() # Start time\n",
    "predictions2 = [y for x in [Xtest_tfidf[i:i+2000,:] for i in range(0,Xtest_tfidf.shape[0],2000)] for y in voting_model_tfidf.predict(x)]\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"VotingClassifier - Time taken for prediction:\", elapsed, \"seconds\")\n",
    "cr2=classification_report(Ytest, predictions2, digits=4)\n",
    "print(cr2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
